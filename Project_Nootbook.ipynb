{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meaZ6nUfztNe"
   },
   "source": [
    "# **308386804**\n",
    "# **204434906**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K5Ov0WCP3St"
   },
   "source": [
    "## **Imports & Installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHwfpK_lfkx6",
    "outputId": "d65464bf-076f-4716-f194-bb84430b0a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-swa\n",
      "  Downloading https://files.pythonhosted.org/packages/07/73/0256256dae8206e239e031a15a61e09f67412e4c176eed8b74c3b2e9cbfe/keras_swa-0.1.6-py3-none-any.whl\n",
      "Installing collected packages: keras-swa\n",
      "Successfully installed keras-swa-0.1.6\n"
     ]
    }
   ],
   "source": [
    "pip install keras-swa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avMK2lu5rdBp"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HGUXLaaZqvNj",
    "outputId": "a37c841d-01da-4b7a-be68-291292c13ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna==0.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/dc/c10b179b9bbee133241049ee360cc3135b725e18380f1b7d5aced02f961e/optuna-0.14.0.tar.gz (91kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 9.1MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.15.0)\n",
      "Collecting cliff\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/11/aea1cacbd4cf8262809c4d6f95dcb3f2802594de1f51c5bd454d69bf15c5/cliff-3.8.0-py3-none-any.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
      "\u001b[?25hCollecting colorlog\n",
      "  Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna==0.14.0) (1.1.5)\n",
      "Collecting alembic\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/80/ef186e599a57d0e4cb78fc76e0bfc2e6953fa9716b2a5cf2de0117ed8eb5/alembic-1.6.5-py2.py3-none-any.whl (164kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 35.5MB/s \n",
      "\u001b[?25hCollecting typing\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (4.6.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==0.14.0) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (3.13)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (2.1.0)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e0/1d4702dd81121d04a477c272d47ee5b6bc970d1a0990b11befa275c55cf2/pbr-5.6.0-py2.py3-none-any.whl (111kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 46.2MB/s \n",
      "\u001b[?25hCollecting cmd2>=1.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/ca/d407811641ec1d8bd8a38ee3165d73aa44776d7700436bd4d4a6606f2736/cmd2-2.1.2-py3-none-any.whl (141kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 46.3MB/s \n",
      "\u001b[?25hCollecting stevedore>=2.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==0.14.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna==0.14.0) (2018.9)\n",
      "Collecting Mako\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.5MB/s \n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna==0.14.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna==0.14.0) (3.5.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna==0.14.0) (0.2.5)\n",
      "Collecting colorama>=0.3.7\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==0.14.0) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==0.14.0) (2.0.1)\n",
      "Building wheels for collected packages: optuna, typing, pyperclip\n",
      "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for optuna: filename=optuna-0.14.0-cp37-none-any.whl size=125710 sha256=be990eb35ca0bb37cd6761e7dd2ad800cd2d7481a74487c8aba9c36b2589af8f\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/5f/2b/3c699d0425d8f34f8a81dfebe30f4d2b9f19377f41c57dbae3\n",
      "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for typing: filename=typing-3.7.4.3-cp37-none-any.whl size=26323 sha256=57c0089b6a9075fab819d5ff84f79f601e41f38cfdc5c058ffac76f5bfe3832f\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11136 sha256=129c146457ede53cd74d3985c099a32d37f412dfd5ee8935ed2b92dc76edc5ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n",
      "Successfully built optuna typing pyperclip\n",
      "Installing collected packages: pbr, colorama, pyperclip, cmd2, stevedore, cliff, colorlog, Mako, python-editor, alembic, typing, optuna\n",
      "Successfully installed Mako-1.1.4 alembic-1.6.5 cliff-3.8.0 cmd2-2.1.2 colorama-0.4.4 colorlog-5.0.1 optuna-0.14.0 pbr-5.6.0 pyperclip-1.8.2 python-editor-1.0.4 stevedore-3.3.0 typing-3.7.4.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "typing"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optkeras==0.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/e3/51dca48c141c8c461a01c0d1fabe33ded992abfe9ec3c0a899e64a7834ab/optkeras-0.0.7-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (1.19.5)\n",
      "Requirement already satisfied: optuna>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (0.14.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from optkeras==0.0.7) (2.4.3)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.8.0)\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (3.7.4.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.1.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.4.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (5.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.15.0)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.6.5)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=0.9.0->optkeras==0.0.7) (1.4.20)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->optkeras==0.0.7) (3.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->optkeras==0.0.7) (3.13)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (5.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (2.4.7)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (2.1.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (3.3.0)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=0.9.0->optkeras==0.0.7) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->optuna>=0.9.0->optkeras==0.0.7) (2.8.1)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (1.1.4)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=0.9.0->optkeras==0.0.7) (1.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (4.6.1)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras->optkeras==0.0.7) (1.5.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna>=0.9.0->optkeras==0.0.7) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (21.2.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (1.8.2)\n",
      "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=0.9.0->optkeras==0.0.7) (3.7.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna>=0.9.0->optkeras==0.0.7) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna>=0.9.0->optkeras==0.0.7) (3.5.0)\n",
      "Installing collected packages: optkeras\n",
      "Successfully installed optkeras-0.0.7\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google import colab\n",
    "    !pip3 install optuna==0.14.0\n",
    "    !pip install optkeras==0.0.7\n",
    "except:\n",
    "    print('Run in non-Colab environment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1ohAGPS-eON",
    "outputId": "f3fc53ed-af14-4f62-c68e-03fd92b6f474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rFce_Sl_JM2z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import timeit\n",
    "import random\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import friedmanchisquare\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from swa.tfkeras import SWA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, Activation, BatchNormalization, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import scikit_posthocs as sp\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "from optkeras.optkeras import OptKeras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zutdgcbXPflT"
   },
   "source": [
    "## **Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6BOi-34tdtZ"
   },
   "outputs": [],
   "source": [
    "path = r'/content/drive/MyDrive/classification_datasets'\n",
    "output_path = r'/content/drive/MyDrive/ML_Project'\n",
    "\n",
    "train_nes_X = ''\n",
    "train_nes_Y = ''\n",
    "valid_X = ''\n",
    "valid_Y = ''\n",
    "N_FEATURES = ''\n",
    "N_CLASSES = ''\n",
    "\n",
    "#This is The index of the Full_DF Table - start 1 -> 600\n",
    "Iter = 1\n",
    "\n",
    "Dataset_names = ['abalon', 'wall-following' , 'car' , 'wine-quality-white' , 'steel-plates' , 'yeast' , 'annealing' , 'wine-quality-red' , 'waveform' , 'statlog-image',\n",
    "         'cardiotocography-3clases' , 'blood' , 'monks-1' , 'credit-approval' , 'pima' , 'musk-1' , 'mushroom' , 'chess-krvkp' , 'diabetes' , 'pm10']\n",
    "Binary_datasets = ['blood' , 'monks-1' , 'credit-approval' , 'pima' , 'musk-1' , 'mushroom' , 'chess-krvkp' , 'diabetes' , 'pm10']\n",
    "\n",
    "Full_DF = pd.DataFrame(columns=['Dataset Name' , 'Algorithm Name', 'Cross Validation [1-10]', 'Hyper-Parameters Values',\n",
    "                                'Accuracy', 'TPR', 'FPR', 'Precision', 'AUC', 'PR-Curve', 'Training Time', 'Inference Time'])\n",
    "\n",
    "OK = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwkiikpVxwnV"
   },
   "outputs": [],
   "source": [
    "def Delete_All_H5_Files():\n",
    "    files_in_directory = os.listdir(r'/content')\n",
    "    filtered_files = [file for file in files_in_directory if file.endswith(\".h5\")]\n",
    "    for file in filtered_files:\n",
    "        path_to_file = os.path.join(r'/content', file)\n",
    "        os.remove(path_to_file)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmRVRqjShjvi"
   },
   "source": [
    "## **Improvement**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEnOCRZjsgg0"
   },
   "source": [
    "### **Cosine Aneealing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Jt0vjiy_hi2b"
   },
   "outputs": [],
   "source": [
    "def cosine_lr(curr_epoch, num_epochs, num_cycles, lr_swa_max):\n",
    "    epochs_per_cycle = int(num_epochs/num_cycles)\n",
    "    cos_inner = (math.pi * (curr_epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "    return lr_swa_max/2 * (math.cos(cos_inner) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "pF-vZxrGhtou",
    "outputId": "87b301f1-2073-4574-ae18-63a00aae2ca2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "n_cycles = 5\n",
    "lrate_max = 0.01\n",
    "series = [cosine_lr(i, n_epochs, n_cycles, lrate_max) for i in range(n_epochs)]\n",
    "plt.plot(series, color='plum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAc19K7OtOBu"
   },
   "source": [
    "### **SWA Callback With Our Improvement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2433mSwlhzs6"
   },
   "outputs": [],
   "source": [
    "\"\"\" SWA Improved Callback\n",
    "\"\"\"\n",
    "def create_swa_callback_class(K, Callback, BatchNormalization):\n",
    "    \"\"\"Injecting library dependencies\"\"\"\n",
    "    class SWA_Improved(Callback):\n",
    "        \"\"\"Stochastic Weight Averging.\n",
    "        # Paper\n",
    "            title: Averaging Weights Leads to Wider Optima and Better Generalization\n",
    "            link: https://arxiv.org/abs/1803.05407\n",
    "\n",
    "        # Arguments\n",
    "            start_epoch:   integer, epoch when swa should start.\n",
    "            lr_schedule:   'cosin' - string, type of learning rate schedule.\n",
    "            swa_lr:        float, start learning rate for swa.\n",
    "            swa_lr2:       float, upper bound of cosin learning rate.\n",
    "            swa_freq:      integer, length of learning rate cycle.\n",
    "            batch_size     integer, batch size (for batch norm with generator)\n",
    "            verbose:       integer, verbosity mode, 0 or 1.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            start_epoch,\n",
    "            lr_schedule=\"cosin\",\n",
    "            swa_lr=\"auto\",\n",
    "            swa_lr2=\"auto\",\n",
    "            swa_freq=1,\n",
    "            batch_size=None,\n",
    "            verbose=0,\n",
    "        ):\n",
    "            super(SWA_Improved, self).__init__()\n",
    "            self.start_epoch = start_epoch - 1\n",
    "            self.lr_schedule = 'cosin'\n",
    "            self.swa_lr = swa_lr\n",
    "\n",
    "            # if no user determined upper bound, make one based off of the lower bound\n",
    "            self.swa_lr2 = swa_lr2 if swa_lr2 is not None else 10 * swa_lr\n",
    "            self.swa_freq = swa_freq\n",
    "            self.batch_size = batch_size\n",
    "            self.verbose = verbose\n",
    "\n",
    "            if start_epoch < 2:\n",
    "                raise ValueError('\"swa_start\" attribute cannot be lower than 2.')\n",
    "            schedules = [\"cosin\"]\n",
    "            if self.lr_schedule not in schedules:\n",
    "                raise ValueError('\"{}\" is not a valid learning rate schedule'.format(self.lr_schedule))\n",
    "\n",
    "            if self.lr_schedule == \"cosin\" and self.swa_freq < 2:\n",
    "                raise ValueError('\"swa_freq\" must be higher than 1 for cyclic schedule.')\n",
    "\n",
    "            if self.swa_lr == \"auto\" and self.swa_lr2 != \"auto\":\n",
    "                raise ValueError('\"swa_lr2\" cannot be manually set if \"swa_lr\" is automatic.')\n",
    "\n",
    "            if (self.swa_lr != \"auto\" and self.swa_lr2 != \"auto\" and self.swa_lr > self.swa_lr2):\n",
    "                raise ValueError('\"swa_lr\" must be lower than \"swa_lr2\".')\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.lr_record = []\n",
    "            self.epochs = self.params.get(\"epochs\")\n",
    "            self.n_cycles = self.epochs / self.swa_freq\n",
    "            if self.start_epoch >= self.epochs - 1:\n",
    "                raise ValueError('\"swa_start\" attribute must be lower than \"epochs\".')\n",
    "            self.init_lr = K.eval(self.model.optimizer.lr)\n",
    "            # automatic swa_lr\n",
    "            if self.swa_lr == \"auto\":\n",
    "                self.swa_lr = 0.1 * self.init_lr\n",
    "            if self.init_lr < self.swa_lr:\n",
    "                raise ValueError('\"swa_lr\" must be lower than rate set in optimizer.')\n",
    "            # automatic swa_lr2 between initial lr and swa_lr\n",
    "            if self.lr_schedule == \"cosin\" and self.swa_lr2 == \"auto\":\n",
    "                self.swa_lr2 = self.swa_lr + (self.init_lr - self.swa_lr) * 0.25\n",
    "            self._check_batch_norm()\n",
    "            if self.has_batch_norm and self.batch_size is None:\n",
    "                raise ValueError('\"batch_size\" needs to be set for models with batch normalization layers.')\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.current_epoch = epoch\n",
    "            self._scheduler(epoch)\n",
    "            if self.is_swa_start_epoch: ## when we get to start epoch we save the pre-train model weights in W_swa\n",
    "                self.swa_weights = self.model.get_weights()\n",
    "                if self.verbose > 0:\n",
    "                    print(\"\\nEpoch %05d: starting stochastic weight averaging\" % (epoch + 1))\n",
    "            if self.is_batch_norm_epoch: ## when it batch normalization epoch we will update the W model weights with W_swa weights so far\n",
    "                self._set_swa_weights(epoch)\n",
    "                if self.verbose > 0:\n",
    "                    print(\"\\nEpoch %05d: reinitializing batch normalization layers\" % (epoch + 1))\n",
    "                self._reset_batch_norm()\n",
    "                if self.verbose > 0:\n",
    "                  print(\"\\nEpoch %05d: running forward pass to adjust batch normalization\" % (epoch + 1))\n",
    "\n",
    "        def on_batch_begin(self, batch, logs=None):\n",
    "            # update lr each batch for cosin lr schedule\n",
    "            if self.lr_schedule == \"cosin\":\n",
    "                self._update_lr(self.current_epoch, batch)\n",
    "            if self.is_batch_norm_epoch:\n",
    "                batch_size = self.batch_size\n",
    "                momentum = batch_size / (batch * batch_size + batch_size)\n",
    "                for layer in self.batch_norm_layers:\n",
    "                    layer.momentum = momentum\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if self.is_swa_start_epoch: ## if we start SWA\n",
    "                self.swa_start_epoch = epoch\n",
    "            if self.is_swa_epoch and not self.is_batch_norm_epoch: ## if this is a SWA epoch and cycle is ending we need to update W_swa weights by averaging all the cycle ensamble models \n",
    "                self.swa_weights = self._average_weights(epoch)\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            if not self.has_batch_norm:\n",
    "                self._set_swa_weights(self.epochs)\n",
    "            else:\n",
    "                self._restore_batch_norm()\n",
    "            for batch_lr in self.lr_record:\n",
    "                self.model.history.history.setdefault(\"lr\", []).append(batch_lr)\n",
    "\n",
    "        def _scheduler(self, epoch):\n",
    "            swa_epoch = epoch - self.start_epoch # get the current swa epoch, and\n",
    "            self.is_swa_epoch = (epoch >= self.start_epoch and swa_epoch % self.swa_freq == 0)  ##check if it after the start epoch and if it is the end of cycle\n",
    "            self.is_swa_start_epoch = epoch == self.start_epoch ## check if this is the swa start epoch\n",
    "            self.is_batch_norm_epoch = epoch == self.epochs - 1 and self.has_batch_norm ## check if this is a batch_norm epoch\n",
    "\n",
    "        def _average_weights(self, epoch):\n",
    "              return [ ( swa_w * ( (epoch - self.start_epoch) / self.swa_freq) + w ) / ( ( epoch - self.start_epoch ) + 1 ) for swa_w, w in zip( self.swa_weights, self.model.get_weights() ) ]\n",
    "\n",
    "        def _update_lr(self, epoch, batch=None):\n",
    "            if self.is_batch_norm_epoch:\n",
    "                lr = 0\n",
    "                K.set_value(self.model.optimizer.lr, lr)\n",
    "            elif self.lr_schedule == \"cosin\":\n",
    "                lr = cosine_lr(epoch, self.epochs, self.swa_freq , self.swa_lr2)\n",
    "                K.set_value(self.model.optimizer.lr, lr)\n",
    "            self.lr_record.append(lr)\n",
    "\n",
    "        # this function calculate the cosin learning rate schedualer\n",
    "        def cosine_lr(curr_epoch, num_epochs, num_cycles, lr_swa_max):\n",
    "            epochs_per_cycle = int(num_epochs/num_cycles)\n",
    "            cos_inner = (math.pi * (curr_epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "            return lr_swa_max/2 * (math.cos(cos_inner) + 1)\n",
    "\n",
    "        ## update the model with the new W_swa weights\n",
    "        def _set_swa_weights(self, epoch):\n",
    "            self.model.set_weights(self.swa_weights)\n",
    "            if self.verbose > 0:\n",
    "                print(\"\\nEpoch %05d: final model weights set to stochastic weight average\" % (epoch + 1))\n",
    "\n",
    "        def _check_batch_norm(self):\n",
    "\n",
    "            self.batch_norm_momentums = []\n",
    "            self.batch_norm_layers = []\n",
    "            self.has_batch_norm = False\n",
    "            self.running_bn_epoch = False\n",
    "\n",
    "            for layer in self.model.layers:\n",
    "                if issubclass(layer.__class__, BatchNormalization):\n",
    "                    self.has_batch_norm = True\n",
    "                    self.batch_norm_momentums.append(layer.momentum)\n",
    "                    self.batch_norm_layers.append(layer)\n",
    "            if self.verbose > 0 and self.has_batch_norm:\n",
    "                print(\"Model uses batch normalization. SWA will require last epoch to be a forward pass and will run with no learning rate\")\n",
    "\n",
    "        def _reset_batch_norm(self):\n",
    "            for layer in self.batch_norm_layers:\n",
    "                # to get properly initialized moving mean and moving variance weights\n",
    "                # we initialize a new batch norm layer from the config of the existing\n",
    "                # layer, build that layer, retrieve its reinitialized moving mean and\n",
    "                # moving var weights and then delete the layer\n",
    "                bn_config = layer.get_config()\n",
    "                new_batch_norm = BatchNormalization(**bn_config)\n",
    "                new_batch_norm.build(layer.input_shape)\n",
    "                new_moving_mean, new_moving_var = new_batch_norm.get_weights()[-2:]\n",
    "                # get rid of the new_batch_norm layer\n",
    "                del new_batch_norm\n",
    "                # get the trained gamma and beta from the current batch norm layer\n",
    "                trained_weights = layer.get_weights()\n",
    "                new_weights = []\n",
    "                # get gamma if exists\n",
    "                if bn_config[\"scale\"]:\n",
    "                    new_weights.append(trained_weights.pop(0))\n",
    "                # get beta if exists\n",
    "                if bn_config[\"center\"]:\n",
    "                    new_weights.append(trained_weights.pop(0))\n",
    "                new_weights += [new_moving_mean, new_moving_var]\n",
    "                # set weights to trained gamma and beta, reinitialized mean and variance\n",
    "                layer.set_weights(new_weights)\n",
    "\n",
    "        def _restore_batch_norm(self):\n",
    "            for layer, momentum in zip(\n",
    "                self.batch_norm_layers, self.batch_norm_momentums\n",
    "            ):\n",
    "                layer.momentum = momentum\n",
    "\n",
    "    return SWA_Improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFq7Zn_LQAZm"
   },
   "source": [
    "## **Main Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zM-Ay7IWJNH0"
   },
   "outputs": [],
   "source": [
    "def CalculateAllScores(fold, y_test, pred_Y, pred_prob_Y, binaryClassifier=False):\n",
    "    global Full_DF, Iter\n",
    "    print(\"CalculateAllScores\")\n",
    "    if binaryClassifier is True:\n",
    "        TN, FP, FN, TP = metrics.confusion_matrix(y_test, pred_Y).ravel()\n",
    "        auc = metrics.roc_auc_score(y_test, pred_prob_Y)\n",
    "        pr_curve = metrics.average_precision_score(y_test, pred_prob_Y)\n",
    "\n",
    "    else:\n",
    "        cm = metrics.confusion_matrix(y_test, pred_Y)\n",
    "        FP = cm.sum(axis=0) - np.diag(cm) \n",
    "        FN = cm.sum(axis=1) - np.diag(cm)\n",
    "        TP = np.diag(cm)\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "        FP = np.mean(FP.astype(float))\n",
    "        FN = np.mean(FN.astype(float))\n",
    "        TP = np.mean(TP.astype(float))\n",
    "        TN = np.mean(TN.astype(float))\n",
    "        auc= metrics.roc_auc_score(y_test, pred_prob_Y, multi_class='ovr')\n",
    "        y_test_hot = pd.get_dummies(y_test.astype(str))\n",
    "        pr_curve = metrics.average_precision_score(y_test_hot, pred_prob_Y)\n",
    "\n",
    "    Full_DF.loc[Iter, 'Accuracy'] = round((TP+TN)/(TP+FP+FN+TN), 3)\n",
    "    Full_DF.loc[Iter, 'TPR'] = round(TP/(TP+FN), 3)\n",
    "    Full_DF.loc[Iter, 'FPR'] = round(FP/(FP+TN), 3)\n",
    "    Full_DF.loc[Iter, 'Precision'] = round(TP/(TP+FP), 3)\n",
    "    Full_DF.loc[Iter, 'AUC'] = round(auc, 3)\n",
    "    Full_DF.loc[Iter, 'PR-Curve'] = round(pr_curve, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iqKYLVEBAeM"
   },
   "outputs": [],
   "source": [
    "def Get_Model(model_name, best_params):\n",
    "    if model_name == 'SGD':\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 ,activation='relu', input_shape=(N_FEATURES,), name='Dense128_Layer'))\n",
    "        model.add(Dense(64 ,activation='relu', input_shape=(N_FEATURES,), name='Dense64_Layer'))\n",
    "        model.add(Dense(32 ,activation='relu', input_shape=(N_FEATURES,), name='Dense32_Layer'))\n",
    "        model.add(Dense(16 ,activation='relu', input_shape=(N_FEATURES,), name='Dense16_Layer'))\n",
    "        model.add(Dropout(best_params[\"dropout_rate\"], name=\"Dropout_Layer\"))\n",
    "        if N_CLASSES == 2:\n",
    "            model.add(Dense(1,activation='sigmoid', name=\"Output_Layer\"))\n",
    "            model.compile(loss='binary_crossentropy', \n",
    "                    optimizer=SGD(learning_rate=best_params[\"learning_rate\"]), \n",
    "                    metrics=['accuracy'])\n",
    "        else:\n",
    "            model.add(Dense(N_CLASSES,activation='softmax', name=\"Output_Layer\"))\n",
    "            model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer=SGD(learning_rate=best_params[\"learning_rate\"]), \n",
    "                    metrics=['accuracy'])\n",
    "        return model\n",
    "    if model_name in ['SWA' , 'SWA_Improved']:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 ,activation='relu', input_shape=(N_FEATURES,), name='Dense128_Layer'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(64 ,activation='relu', input_shape=(N_FEATURES,), name='Dense64_Layer'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(32 ,activation='relu', input_shape=(N_FEATURES,), name='Dense32_Layer'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(16 ,activation='relu', input_shape=(N_FEATURES,), name='Dense16_Layer'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5, name=\"Dropout_Layer\"))\n",
    "        if N_CLASSES == 2:\n",
    "            model.add(Dense(1,activation='sigmoid', name=\"Output_Layer\"))\n",
    "            model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=best_params[\"learning_rate\"]), metrics=['accuracy'])\n",
    "        else:\n",
    "            model.add(Dense(N_CLASSES,activation='softmax', name=\"Output_Layer\"))\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=best_params[\"learning_rate\"]), metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lzm-ANOr1d5"
   },
   "outputs": [],
   "source": [
    "def objectiveSWA(trial):\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128 ,activation='relu', input_shape=(N_FEATURES,), name='Dense128_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64 ,activation='relu', input_shape=(N_FEATURES,), name='Dense64_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32 ,activation='relu', input_shape=(N_FEATURES,), name='Dense32_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(16 ,activation='relu', input_shape=(N_FEATURES,), name='Dense16_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5, name=\"Dropout_Layer\"))\n",
    "    if N_CLASSES == 2:\n",
    "        model.add(Dense(1,activation='sigmoid', name=\"Output_Layer\"))\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=SGD(learning_rate=trial.suggest_loguniform(\"learning_rate\", 1e-3, 3e-3)), \n",
    "                  metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(N_CLASSES,activation='softmax', name=\"Output_Layer\"))\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=SGD(learning_rate=trial.suggest_loguniform(\"learning_rate\",1e-3, 3e-3)), \n",
    "                  metrics=['accuracy'])\n",
    "    swa = SWA(start_epoch=12, lr_schedule='cyclic', \n",
    "            swa_lr=trial.suggest_loguniform(\"swa_lr\", 1e-3, 3e-3), \n",
    "            swa_lr2=trial.suggest_loguniform(\"swa_lr2\",3e-3, 6e-3), \n",
    "            swa_freq=trial.suggest_int(\"swa_freq\", 2, 4), \n",
    "            batch_size=32)\n",
    "    cp = tf.keras.callbacks.ModelCheckpoint('SWA_Model_trial_{}.h5'.format(trial.number), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    if N_CLASSES == 2:\n",
    "        model.fit(train_nes_X, train_nes_Y, validation_data=(valid_X, valid_Y), epochs=18, callbacks=[cp, swa])\n",
    "    else:\n",
    "        model.fit(train_nes_X, to_categorical(train_nes_Y, N_CLASSES), validation_data=(valid_X, to_categorical(valid_Y, N_CLASSES)), epochs=18, callbacks=[cp, swa])\n",
    "    return OK.trial_best_value\n",
    "\n",
    "def objectiveSWAIMP(trial):\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128 ,activation='relu', input_shape=(N_FEATURES,), name='Dense128_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64 ,activation='relu', input_shape=(N_FEATURES,), name='Dense64_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32 ,activation='relu', input_shape=(N_FEATURES,), name='Dense32_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(16 ,activation='relu', input_shape=(N_FEATURES,), name='Dense16_Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5, name=\"Dropout_Layer\"))\n",
    "    if N_CLASSES == 2:\n",
    "        model.add(Dense(1,activation='sigmoid', name=\"Output_Layer\"))\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=SGD(learning_rate=trial.suggest_loguniform(\"learning_rate\", 1e-3, 3e-3)), \n",
    "                  metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(N_CLASSES,activation='softmax', name=\"Output_Layer\"))\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=SGD(learning_rate=trial.suggest_loguniform(\"learning_rate\",1e-2, 3e-2)), \n",
    "                  metrics=['accuracy'])\n",
    "    swa = SWA(start_epoch=12, lr_schedule='cyclic', \n",
    "            swa_lr=trial.suggest_loguniform(\"swa_lr\", 1e-3, 3e-3), \n",
    "            swa_lr2=trial.suggest_loguniform(\"swa_lr2\",3e-3, 6e-3), \n",
    "            swa_freq=trial.suggest_int(\"swa_freq\", 2, 4), \n",
    "            batch_size=32)\n",
    "    cp = tf.keras.callbacks.ModelCheckpoint('SWA_Improved_Model_trial_{}.h5'.format(trial.number), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    if N_CLASSES == 2:\n",
    "        model.fit(train_nes_X, train_nes_Y, validation_data=(valid_X, valid_Y), epochs=18, callbacks=[cp, swa])\n",
    "    else:\n",
    "        model.fit(train_nes_X, to_categorical(train_nes_Y, N_CLASSES), validation_data=(valid_X, to_categorical(valid_Y, N_CLASSES)), epochs=18, callbacks=[cp, swa])\n",
    "    return OK.trial_best_value\n",
    "  \n",
    "def objectiveSGD(trial):\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128 ,activation='relu', input_shape=(N_FEATURES,), name='Dense128_Layer'))\n",
    "    model.add(Dense(64 ,activation='relu', input_shape=(N_FEATURES,), name='Dense64_Layer'))\n",
    "    model.add(Dense(32 ,activation='relu', input_shape=(N_FEATURES,), name='Dense32_Layer'))\n",
    "    model.add(Dense(16 ,activation='relu', input_shape=(N_FEATURES,), name='Dense16_Layer'))\n",
    "    model.add(Dropout(trial.suggest_uniform(\"dropout_rate\", 0.1, 1.0), name=\"Dropout_Layer\"))\n",
    "    if N_CLASSES == 2:\n",
    "        model.add(Dense(1,activation='sigmoid', name=\"Output_Layer\"))\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=SGD(learning_rate=trial.suggest_loguniform(\"learning_rate\",1e-3, 3e-3)), \n",
    "                  metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(N_CLASSES,activation='softmax', name=\"Output_Layer\"))\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=SGD(learning_rate=trial.suggest_loguniform(\"learning_rate\", 1e-3, 3e-3)), \n",
    "                  metrics=['accuracy'])\n",
    "    cp = tf.keras.callbacks.ModelCheckpoint('SGD_Model_trial_{}.h5'.format(trial.number), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    if N_CLASSES == 2:\n",
    "        model.fit(train_nes_X, train_nes_Y, validation_data=(valid_X, valid_Y), epochs=18, callbacks=[cp])\n",
    "    else:\n",
    "        model.fit(train_nes_X, to_categorical(train_nes_Y, N_CLASSES), validation_data=(valid_X, to_categorical(valid_Y, N_CLASSES)), epochs=18, callbacks=[cp])\n",
    "    return OK.trial_best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6p9xjXFhshx"
   },
   "outputs": [],
   "source": [
    "def CrossValidationRandomSearch(model_name, dataset_name, X, Y, binaryClassifier=False):\n",
    "    global train_nes_X, train_nes_Y, valid_X, valid_Y, N_FEATURES, N_CLASSES, Full_DF, Iter\n",
    "    global OK\n",
    "    stratified_kfold = StratifiedKFold(n_splits=10 , shuffle = True , random_state = 9051993)\n",
    "    print(\"## Start 10 Fold Croos Validation ##\")\n",
    "    k = 1\n",
    "    for train_index, test_index in stratified_kfold.split(np.zeros(len(Y)),Y):\n",
    "        Full_DF.loc[Iter, 'Cross Validation [1-10]'] = k\n",
    "        Full_DF.loc[Iter, 'Dataset Name'] = dataset_name\n",
    "        Full_DF.loc[Iter, 'Algorithm Name'] = model_name\n",
    "        print(\"\\tStart Fold Number {}\".format(k))\n",
    "        train_X = X.iloc[train_index]\n",
    "        train_Y = Y.iloc[train_index]\n",
    "        test_X = X.iloc[test_index].values\n",
    "        test_Y = Y.iloc[test_index]\n",
    "        N_FEATURES = train_X.shape[1]\n",
    "        N_CLASSES = len(np.unique(train_Y))\n",
    "        print(np.unique(train_Y) , np.unique(test_Y))\n",
    "        nested_kfold = StratifiedKFold(n_splits=3 , shuffle = True , random_state = 3535656)\n",
    "        start_nested_kfold = timeit.default_timer()\n",
    "        best_params = []\n",
    "        best_values = []\n",
    "        \n",
    "        OK = OptKeras(study_name='3Folds Hyper-Parameters',\n",
    "              monitor='val_acc',\n",
    "              direction='maximize',\n",
    "              random_grid_search_mode=True)\n",
    "\n",
    "        for train_nested_index, valid_index in stratified_kfold.split(np.zeros(len(train_Y)),train_Y):\n",
    "            train_nes_X = train_X.iloc[train_nested_index].values\n",
    "            train_nes_Y = train_Y.iloc[train_nested_index]\n",
    "            valid_X = train_X.iloc[valid_index].values\n",
    "            valid_Y = train_Y.iloc[valid_index]\n",
    "            if model_name == 'SWA_Improved':\n",
    "                OK.random_grid_search(objectiveSWAIMP , n_trials=int(50))\n",
    "            elif model_name == 'SWA':\n",
    "                OK.random_grid_search(objectiveSWA, n_trials=int(50))\n",
    "            else:\n",
    "                OK.random_grid_search(objectiveSGD, n_trials=int(50))\n",
    "            best_params.append(OK.best_trial.params)\n",
    "            best_values.append(OK.best_trial.value)\n",
    "            OK.print_results\n",
    "\n",
    "        stop_nested_kfold = timeit.default_timer()\n",
    "        Full_DF.loc[Iter, 'Training Time'] = stop_nested_kfold - start_nested_kfold\n",
    "        max_acc = max(best_values)\n",
    "        fold_num = best_values.index(max_acc)\n",
    "        s = ''\n",
    "        for key in best_params[fold_num]:\n",
    "            s += \"{}: {} | \".format(key , best_params[fold_num][key])\n",
    "        Full_DF.loc[Iter, 'Hyper-Parameters Values'] = s\n",
    "\n",
    "        best_model = Get_Model(model_name, best_params[fold_num])\n",
    "        best_model.load_weights(\"{}_Model_trial_{}.h5\".format(model_name,OK.best_trial.number))\n",
    "        \n",
    "        start_predict = timeit.default_timer()\n",
    "        if N_CLASSES == 2:\n",
    "            results = best_model.evaluate(test_X, test_Y, N_CLASSES)\n",
    "        else:\n",
    "            results = best_model.evaluate(test_X, to_categorical(test_Y, N_CLASSES))\n",
    "        print(results)\n",
    "        stop_predict = timeit.default_timer()\n",
    "        Full_DF.loc[Iter, 'Inference Time'] = stop_predict - start_predict\n",
    "        pred_prob_Y = best_model.predict_proba(test_X)\n",
    "        pred_Y = Get_Class_From_Probs(pred_prob_Y, binaryClassifier)\n",
    "        CalculateAllScores(k, test_Y, pred_Y, pred_prob_Y, binaryClassifier)\n",
    "        Delete_All_H5_Files()\n",
    "        Iter += 1\n",
    "        k += 1\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8ne4NtQNHMF"
   },
   "outputs": [],
   "source": [
    "def Get_Class_From_Probs(pred_prob_Y, isBinary):\n",
    "    if isBinary:\n",
    "        rounded = [round(x[0]) for x in pred_prob_Y]\n",
    "        return rounded\n",
    "    df = pd.DataFrame(pred_prob_Y)\n",
    "    df['Max'] = df.idxmax(axis=1)\n",
    "    return df['Max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkoyY78xSUCu"
   },
   "outputs": [],
   "source": [
    "def Import_DataSets_To_DataFrame(df_name):\n",
    "    current_path = os.path.join(path, df_name + \".csv\")\n",
    "    df = pd.read_csv(current_path)\n",
    "    df.sample(frac=1)\n",
    "    X = df[df.columns[:-1]]\n",
    "    Y = df.iloc[: , -1:]\n",
    "    return df_name,X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQiPWyrOQN-V"
   },
   "source": [
    "## **Running All Datasets - Each Cell Run 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Nn-3PJnS24N"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for n in Dataset_names:\n",
    "    isBinary=False\n",
    "    name,X,y = Import_DataSets_To_DataFrame(n)\n",
    "    if n in Binary_datasets:\n",
    "        isBinary=True\n",
    "    if n == 'diabetes':\n",
    "        y.replace({\"tested_positive\": 0, \"tested_negative\": 1}, inplace=True)\n",
    "    if n == 'pm10':\n",
    "        y.replace({\"P\": 0, \"N\": 1}, inplace=True)\n",
    "    print(\"\\n##########################################################\\n\\t{}\\n##########################################################\\n\".format(name))\n",
    "    print(y.shape , X.shape)\n",
    "    if len(X) >= 3000:\n",
    "        X, xx, y, yy = train_test_split(X, y, test_size=0.5, random_state=15482)\n",
    "    Details_swa_imp = CrossValidationRandomSearch('SGD', name, X , y, binaryClassifier=isBinary)\n",
    "    Details_swa_imp = CrossValidationRandomSearch('SWA', name, X , y, binaryClassifier=isBinary)\n",
    "    Details_swa_imp = CrossValidationRandomSearch('SWA_Improved', name, X , y, binaryClassifier=isBinary)\n",
    "    Full_DF.to_csv(os.path.join(output_path, 'Full_DF_{}.csv'.format(name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECmvzAw7r3XN"
   },
   "source": [
    "## **Friedman Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6EgF6DCr2eT"
   },
   "outputs": [],
   "source": [
    "df_all_avg = df.groupby(['Dataset Name' , 'Algorithm Name'])['AUC'].mean().reset_index(name='avg_AUC')\n",
    "df_SGD = df_all_avg[df_all_avg['Algorithm Name']== 'SGD']\n",
    "df_SWA = df_all_avg[df_all_avg['Algorithm Name']== 'SWA']\n",
    "df_SWA_Imp = df_all_avg[df_all_avg['Algorithm Name']== 'SWA_Improved']\n",
    "df_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WA8tezhYr7x5"
   },
   "outputs": [],
   "source": [
    "df_SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix-FLu_Jr87L"
   },
   "outputs": [],
   "source": [
    "df_SWA_Imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK3Dud16RUHG",
    "outputId": "6e92fa1e-9162-4e79-9bda-debb060277d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics=24.700, p=0.000\n",
      "Different distributions (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# Friedman test\n",
    "\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "\n",
    "# compare samples\n",
    "stat, p = friedmanchisquare(df_SGD['avg_AUC'], df_SWA['avg_AUC'], df_SWA_Imp['avg_AUC'])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liMGfDJisEe8"
   },
   "source": [
    "## **Post Hoc Neime Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "6n0d18iRQ2n6",
    "outputId": "d62091b5-074d-4ec0-c57b-050e64ae1d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-posthocs in /usr/local/lib/python3.7/dist-packages (0.6.7)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scikit-posthocs) (0.10.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scikit-posthocs) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-posthocs) (1.1.5)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scikit-posthocs) (0.11.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from scikit-posthocs) (3.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scikit-posthocs) (1.19.5)\n",
      "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scikit-posthocs) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit-posthocs) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit-posthocs) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit-posthocs) (0.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.0->statsmodels->scikit-posthocs) (1.15.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.781714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.781714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2\n",
       "0  1.000  0.001000  0.001000\n",
       "1  0.001  1.000000  0.781714\n",
       "2  0.001  0.781714  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#combine three groups into one array\n",
    "data = np.array([df_SGD['avg_AUC'], df_SWA['avg_AUC'], df_SWA_Imp['avg_AUC']])\n",
    "\n",
    "#perform Nemenyi post-hoc test\n",
    "sp.posthoc_nemenyi_friedman(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmE38JloTu5a"
   },
   "outputs": [],
   "source": [
    "# Note: We had to transpose the numpy array (data.T) in order to perform the post-hoc test correctly.\n",
    "\n",
    "# The Nemeyi post-hoc test returns the p-values for each pairwise comparison of means. From the output we can see the following p-values:\n",
    "\n",
    "# P-value of group 0 vs. group 1: 0.001\n",
    "# P-value of group 0 vs. group 2: 0.001\n",
    "# P-value of group 1 vs. group 2: 0.7817\n",
    "\n",
    "# At α = .05, the only two groups that have statistically significantly different means are group 1 and group 2.\n",
    "\n",
    "# Note: The Nemenyi test converted the group number from 1, 2, 3 into 0, 1, 2. Thus, the groups from the original data that are significantly different are groups 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3kQN0PVLUFBD",
    "outputId": "b6cb5ac2-77f4-436f-f0dc-033c427057f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>AUC</th>\n",
       "      <th>PR-Curve</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Inference Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Algorithm Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">abalon</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.7594</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.6026</td>\n",
       "      <td>189.177879</td>\n",
       "      <td>0.324497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.7626</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.8278</td>\n",
       "      <td>0.7018</td>\n",
       "      <td>947.922461</td>\n",
       "      <td>0.535491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.7583</td>\n",
       "      <td>0.6374</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.8243</td>\n",
       "      <td>0.7010</td>\n",
       "      <td>347.188465</td>\n",
       "      <td>0.404932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">annealing</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.8804</td>\n",
       "      <td>0.7607</td>\n",
       "      <td>0.0797</td>\n",
       "      <td>0.5007</td>\n",
       "      <td>0.2962</td>\n",
       "      <td>155.442032</td>\n",
       "      <td>0.314779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8960</td>\n",
       "      <td>0.7922</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.5392</td>\n",
       "      <td>333.771366</td>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9060</td>\n",
       "      <td>0.8124</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.8595</td>\n",
       "      <td>0.6351</td>\n",
       "      <td>262.662107</td>\n",
       "      <td>0.388661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">blood</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>143.880156</td>\n",
       "      <td>0.271395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.7635</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.4569</td>\n",
       "      <td>310.135687</td>\n",
       "      <td>0.411998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.6206</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>320.700098</td>\n",
       "      <td>0.534545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">car</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.3116</td>\n",
       "      <td>116.849082</td>\n",
       "      <td>0.233531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>0.8263</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.9155</td>\n",
       "      <td>0.6947</td>\n",
       "      <td>216.359921</td>\n",
       "      <td>0.393195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.9513</td>\n",
       "      <td>341.173995</td>\n",
       "      <td>0.437133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cardiotocography-3clases</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.8562</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>0.4086</td>\n",
       "      <td>208.300265</td>\n",
       "      <td>0.431383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.9085</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.9179</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>358.638000</td>\n",
       "      <td>0.580247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9210</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.9562</td>\n",
       "      <td>0.8517</td>\n",
       "      <td>272.139993</td>\n",
       "      <td>0.405737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">chess-krvkp</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.7058</td>\n",
       "      <td>0.3991</td>\n",
       "      <td>0.7159</td>\n",
       "      <td>0.7323</td>\n",
       "      <td>204.198963</td>\n",
       "      <td>0.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8861</td>\n",
       "      <td>0.9141</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.9556</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>422.912491</td>\n",
       "      <td>0.742345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.8788</td>\n",
       "      <td>0.8973</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.9462</td>\n",
       "      <td>0.9441</td>\n",
       "      <td>337.914798</td>\n",
       "      <td>0.561552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">credit-approval</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.6365</td>\n",
       "      <td>0.8118</td>\n",
       "      <td>0.5831</td>\n",
       "      <td>0.7145</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>147.056890</td>\n",
       "      <td>0.363076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8060</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>265.234417</td>\n",
       "      <td>0.453382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.8103</td>\n",
       "      <td>0.8356</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.8737</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>246.846180</td>\n",
       "      <td>0.588629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">diabetes</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.6311</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.6723</td>\n",
       "      <td>145.949709</td>\n",
       "      <td>0.345622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.6912</td>\n",
       "      <td>0.6720</td>\n",
       "      <td>0.2721</td>\n",
       "      <td>0.7672</td>\n",
       "      <td>0.8403</td>\n",
       "      <td>311.051905</td>\n",
       "      <td>0.504392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.6703</td>\n",
       "      <td>0.6480</td>\n",
       "      <td>0.2876</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.8425</td>\n",
       "      <td>269.021140</td>\n",
       "      <td>0.484657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">monks-1</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.5644</td>\n",
       "      <td>0.6562</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.6181</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>162.754193</td>\n",
       "      <td>0.371452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>0.7371</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>266.373538</td>\n",
       "      <td>0.505213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.6387</td>\n",
       "      <td>0.6987</td>\n",
       "      <td>0.4197</td>\n",
       "      <td>0.7090</td>\n",
       "      <td>0.7219</td>\n",
       "      <td>189.953206</td>\n",
       "      <td>0.361654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">mushroom</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.9258</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>302.251480</td>\n",
       "      <td>0.548844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>580.265713</td>\n",
       "      <td>1.017440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>532.472648</td>\n",
       "      <td>0.818809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">musk-1</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.6173</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>0.6835</td>\n",
       "      <td>0.6148</td>\n",
       "      <td>147.395310</td>\n",
       "      <td>0.359246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.6806</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.7567</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>218.645195</td>\n",
       "      <td>0.403216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.7179</td>\n",
       "      <td>0.6401</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.7699</td>\n",
       "      <td>0.7261</td>\n",
       "      <td>237.726211</td>\n",
       "      <td>0.509284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">pima</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.6626</td>\n",
       "      <td>0.1311</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.5529</td>\n",
       "      <td>0.4673</td>\n",
       "      <td>145.578513</td>\n",
       "      <td>0.366516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.5526</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.7767</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>291.205155</td>\n",
       "      <td>0.480134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.7133</td>\n",
       "      <td>0.5569</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>0.6349</td>\n",
       "      <td>318.557285</td>\n",
       "      <td>0.582985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">pm10</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.5020</td>\n",
       "      <td>0.3544</td>\n",
       "      <td>0.3526</td>\n",
       "      <td>0.4845</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>134.140171</td>\n",
       "      <td>0.300492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.4736</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>234.494705</td>\n",
       "      <td>0.495893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.4820</td>\n",
       "      <td>0.5370</td>\n",
       "      <td>0.4730</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>205.621481</td>\n",
       "      <td>0.386349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">statlog-image</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.4209</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>0.8222</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>169.372797</td>\n",
       "      <td>0.292269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.9631</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.9341</td>\n",
       "      <td>346.563118</td>\n",
       "      <td>0.490845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9716</td>\n",
       "      <td>309.327927</td>\n",
       "      <td>0.446406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">steel-plates</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>164.039533</td>\n",
       "      <td>0.313204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.9079</td>\n",
       "      <td>0.6773</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>0.7091</td>\n",
       "      <td>513.349371</td>\n",
       "      <td>0.504082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9214</td>\n",
       "      <td>0.7256</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.9328</td>\n",
       "      <td>0.7933</td>\n",
       "      <td>309.770769</td>\n",
       "      <td>0.395611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">wall-following</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.7626</td>\n",
       "      <td>0.5252</td>\n",
       "      <td>0.1583</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.4151</td>\n",
       "      <td>139.528389</td>\n",
       "      <td>0.248053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8454</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>303.704176</td>\n",
       "      <td>0.381726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.8273</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.9520</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>437.740801</td>\n",
       "      <td>0.438176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">waveform</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.7294</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.7886</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>149.642486</td>\n",
       "      <td>0.292376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8526</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.9207</td>\n",
       "      <td>0.8532</td>\n",
       "      <td>333.456074</td>\n",
       "      <td>0.480645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.8977</td>\n",
       "      <td>0.8464</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.9602</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>365.399002</td>\n",
       "      <td>0.388715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">wine-quality-red</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.8393</td>\n",
       "      <td>0.5180</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.2408</td>\n",
       "      <td>206.522401</td>\n",
       "      <td>0.316341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8545</td>\n",
       "      <td>0.5628</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>0.3409</td>\n",
       "      <td>458.126298</td>\n",
       "      <td>0.489220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.8688</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.7364</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>326.317019</td>\n",
       "      <td>0.427227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">wine-quality-white</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.7779</td>\n",
       "      <td>0.4445</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.5751</td>\n",
       "      <td>0.2749</td>\n",
       "      <td>249.786476</td>\n",
       "      <td>0.320876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8023</td>\n",
       "      <td>0.5062</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.7091</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>546.380247</td>\n",
       "      <td>0.574328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.8215</td>\n",
       "      <td>0.5538</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.7609</td>\n",
       "      <td>0.4316</td>\n",
       "      <td>368.302653</td>\n",
       "      <td>0.410733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">yeast</th>\n",
       "      <th>SGD</th>\n",
       "      <td>0.8366</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.0933</td>\n",
       "      <td>0.5263</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>112.013656</td>\n",
       "      <td>0.239659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA</th>\n",
       "      <td>0.8722</td>\n",
       "      <td>0.4881</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.7347</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>210.429681</td>\n",
       "      <td>0.390112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWA_Improved</th>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.5768</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.8426</td>\n",
       "      <td>0.5404</td>\n",
       "      <td>326.629640</td>\n",
       "      <td>0.450413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Accuracy  ...  Inference Time\n",
       "Dataset Name             Algorithm Name            ...                \n",
       "abalon                   SGD               0.7594  ...        0.324497\n",
       "                         SWA               0.7626  ...        0.535491\n",
       "                         SWA_Improved      0.7583  ...        0.404932\n",
       "annealing                SGD               0.8804  ...        0.314779\n",
       "                         SWA               0.8960  ...        0.493562\n",
       "                         SWA_Improved      0.9060  ...        0.388661\n",
       "blood                    SGD               0.7512  ...        0.271395\n",
       "                         SWA               0.7635  ...        0.411998\n",
       "                         SWA_Improved      0.7514  ...        0.534545\n",
       "car                      SGD               0.8449  ...        0.233531\n",
       "                         SWA               0.9133  ...        0.393195\n",
       "                         SWA_Improved      0.9790  ...        0.437133\n",
       "cardiotocography-3clases SGD               0.8562  ...        0.431383\n",
       "                         SWA               0.9085  ...        0.580247\n",
       "                         SWA_Improved      0.9210  ...        0.405737\n",
       "chess-krvkp              SGD               0.6552  ...        0.429200\n",
       "                         SWA               0.8861  ...        0.742345\n",
       "                         SWA_Improved      0.8788  ...        0.561552\n",
       "credit-approval          SGD               0.6365  ...        0.363076\n",
       "                         SWA               0.8060  ...        0.453382\n",
       "                         SWA_Improved      0.8103  ...        0.588629\n",
       "diabetes                 SGD               0.6311  ...        0.345622\n",
       "                         SWA               0.6912  ...        0.504392\n",
       "                         SWA_Improved      0.6703  ...        0.484657\n",
       "monks-1                  SGD               0.5644  ...        0.371452\n",
       "                         SWA               0.6369  ...        0.505213\n",
       "                         SWA_Improved      0.6387  ...        0.361654\n",
       "mushroom                 SGD               0.9258  ...        0.548844\n",
       "                         SWA               0.9912  ...        1.017440\n",
       "                         SWA_Improved      0.9950  ...        0.818809\n",
       "musk-1                   SGD               0.6173  ...        0.359246\n",
       "                         SWA               0.7009  ...        0.403216\n",
       "                         SWA_Improved      0.7179  ...        0.509284\n",
       "pima                     SGD               0.6626  ...        0.366516\n",
       "                         SWA               0.7042  ...        0.480134\n",
       "                         SWA_Improved      0.7133  ...        0.582985\n",
       "pm10                     SGD               0.5020  ...        0.300492\n",
       "                         SWA               0.4720  ...        0.495893\n",
       "                         SWA_Improved      0.4700  ...        0.386349\n",
       "statlog-image            SGD               0.8345  ...        0.292269\n",
       "                         SWA               0.9631  ...        0.490845\n",
       "                         SWA_Improved      0.9753  ...        0.446406\n",
       "steel-plates             SGD               0.8500  ...        0.313204\n",
       "                         SWA               0.9079  ...        0.504082\n",
       "                         SWA_Improved      0.9214  ...        0.395611\n",
       "wall-following           SGD               0.7626  ...        0.248053\n",
       "                         SWA               0.8454  ...        0.381726\n",
       "                         SWA_Improved      0.9137  ...        0.438176\n",
       "waveform                 SGD               0.7294  ...        0.292376\n",
       "                         SWA               0.8526  ...        0.480645\n",
       "                         SWA_Improved      0.8977  ...        0.388715\n",
       "wine-quality-red         SGD               0.8393  ...        0.316341\n",
       "                         SWA               0.8545  ...        0.489220\n",
       "                         SWA_Improved      0.8688  ...        0.427227\n",
       "wine-quality-white       SGD               0.7779  ...        0.320876\n",
       "                         SWA               0.8023  ...        0.574328\n",
       "                         SWA_Improved      0.8215  ...        0.410733\n",
       "yeast                    SGD               0.8366  ...        0.239659\n",
       "                         SWA               0.8722  ...        0.390112\n",
       "                         SWA_Improved      0.9059  ...        0.450413\n",
       "\n",
       "[60 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_avg = df.groupby(['Dataset Name' , 'Algorithm Name']).agg({'Accuracy':'mean', 'TPR' : 'mean', 'FPR': 'mean', 'AUC' : 'mean','PR-Curve':'mean','Training Time':'mean',\t'Inference Time':'mean'})\n",
    "df_all_avg.to_csv(\"avg_of_all_datasets.csv\")\n",
    "df_all_avg"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Project_final_(1)_(1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
